{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d16b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enb import create_dataset_from_json, Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10cf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "train, test = create_dataset_from_json(\"data/fake_newsgroup.json\")\n",
    "\n",
    "categories = set(t['label'] for t in train)\n",
    "c = Classifier(categories)\n",
    "for t in train:\n",
    "    c.train(t['label'], t['text'])\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "for t in test:\n",
    "    probs = c.classify(t['text'])\n",
    "    predicted = max(probs, key=probs.get)\n",
    "    results.append((t['label'], predicted))\n",
    "\n",
    "accuracy(results)\n",
    "\n",
    "    # probs = [(k, round(v, 4)) for k, v in probs.items()]\n",
    "    # sorted_probs = sorted(probs, key=lambda x: x[1], reverse=True)\n",
    "    # print(t['label'], sorted_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9affb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classifier Stats ===\n",
      "Total documents: 8\n",
      "Vocabulary size: 137\n",
      "\n",
      "Number of docs per category {'comp.graphics': 2, 'sci.space': 2, 'rec.sport.baseball': 2, 'talk.politics.guns': 2}\n",
      "Category: rec.sport.baseball\n",
      "  Documents: 2\n",
      "  Prior P(rec.sport.baseball): 0.250\n",
      "  Total tokens: 38\n",
      "  Top 100 words:\n",
      "    been            2\n",
      "    have            2\n",
      "    the             1\n",
      "    team            1\n",
      "    s               1\n",
      "    bullpen         1\n",
      "    has             1\n",
      "    struggling      1\n",
      "    this            1\n",
      "    season          1\n",
      "    several         1\n",
      "    late            1\n",
      "    inning          1\n",
      "    losses          1\n",
      "    could           1\n",
      "    avoided         1\n",
      "    with            1\n",
      "    better          1\n",
      "    relief          1\n",
      "    pitching        1\n",
      "    advanced        1\n",
      "    statistics      1\n",
      "    like            1\n",
      "    war             1\n",
      "    and             1\n",
      "    ops             1\n",
      "    changed         1\n",
      "    how             1\n",
      "    teams           1\n",
      "    evaluate        1\n",
      "    players         1\n",
      "    compared        1\n",
      "    to              1\n",
      "    traditional     1\n",
      "    batting         1\n",
      "    averages        1\n",
      "\n",
      "Category: talk.politics.guns\n",
      "  Documents: 2\n",
      "  Prior P(talk.politics.guns): 0.250\n",
      "  Total tokens: 41\n",
      "  Top 100 words:\n",
      "    the             2\n",
      "    gun             2\n",
      "    debate          1\n",
      "    around          1\n",
      "    control         1\n",
      "    often           1\n",
      "    focuses         1\n",
      "    on              1\n",
      "    balancing       1\n",
      "    individual      1\n",
      "    rights          1\n",
      "    with            1\n",
      "    public          1\n",
      "    safety          1\n",
      "    recent          1\n",
      "    legislation     1\n",
      "    has             1\n",
      "    reignited       1\n",
      "    this            1\n",
      "    discussion      1\n",
      "    some            1\n",
      "    argue           1\n",
      "    that            1\n",
      "    stricter        1\n",
      "    background      1\n",
      "    checks          1\n",
      "    could           1\n",
      "    reduce          1\n",
      "    violence        1\n",
      "    while           1\n",
      "    others          1\n",
      "    believe         1\n",
      "    enforcement     1\n",
      "    of              1\n",
      "    existing        1\n",
      "    laws            1\n",
      "    is              1\n",
      "    real            1\n",
      "    issue           1\n",
      "\n",
      "Category: comp.graphics\n",
      "  Documents: 2\n",
      "  Prior P(comp.graphics): 0.250\n",
      "  Total tokens: 47\n",
      "  Top 100 words:\n",
      "    i               3\n",
      "    am              3\n",
      "    trying          1\n",
      "    to              1\n",
      "    understand      1\n",
      "    how             1\n",
      "    ray             1\n",
      "    tracing         1\n",
      "    differs         1\n",
      "    from            1\n",
      "    rasterization   1\n",
      "    most            1\n",
      "    tutorials       1\n",
      "    explain         1\n",
      "    the             1\n",
      "    math            1\n",
      "    but             1\n",
      "    confused        1\n",
      "    about           1\n",
      "    performance     1\n",
      "    trade           1\n",
      "    offs            1\n",
      "    on              1\n",
      "    modern          1\n",
      "    gpus            1\n",
      "    does            1\n",
      "    anyone          1\n",
      "    have            1\n",
      "    experience      1\n",
      "    with            1\n",
      "    opengl          1\n",
      "    shaders         1\n",
      "    for             1\n",
      "    real            1\n",
      "    time            1\n",
      "    lighting        1\n",
      "    specifically    1\n",
      "    interested      1\n",
      "    in              1\n",
      "    implementing    1\n",
      "    phong           1\n",
      "    shading         1\n",
      "    efficiently     1\n",
      "\n",
      "Category: sci.space\n",
      "  Documents: 2\n",
      "  Prior P(sci.space): 0.250\n",
      "  Total tokens: 42\n",
      "  Top 100 words:\n",
      "    the             3\n",
      "    to              3\n",
      "    space           2\n",
      "    are             2\n",
      "    james           1\n",
      "    webb            1\n",
      "    telescope       1\n",
      "    has             1\n",
      "    significantly   1\n",
      "    improved        1\n",
      "    our             1\n",
      "    ability         1\n",
      "    study           1\n",
      "    exoplanet       1\n",
      "    atmospheres     1\n",
      "    infrared        1\n",
      "    instruments     1\n",
      "    producing       1\n",
      "    incredible      1\n",
      "    data            1\n",
      "    what            1\n",
      "    main            1\n",
      "    engineering     1\n",
      "    challenges      1\n",
      "    of              1\n",
      "    long            1\n",
      "    duration        1\n",
      "    missions        1\n",
      "    mars            1\n",
      "    radiation       1\n",
      "    exposure        1\n",
      "    seems           1\n",
      "    be              1\n",
      "    a               1\n",
      "    major           1\n",
      "    issue           1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.stats(top_n=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explainable-naive-bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
